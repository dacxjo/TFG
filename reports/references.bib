@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
  CTLuse_forced_etal = "yes",
  CTLmax_names_forced_etal = "6",
  CTLnames_show_etal = "1"
}

@misc{vaswani_attention_2023,
    title = {Attention {Is} {All} {You} {Need}},
    url = {http://arxiv.org/abs/1706.03762},
    doi = {10.48550/arXiv.1706.03762},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
    language = {en},
    urldate = {2025-03-07},
    publisher = {arXiv},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    month = aug,
    year = {2023},
    note = {arXiv:1706.03762 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}
@article{kim_global_2025,
    title = {Global patterns and trends in breast cancer incidence and mortality across 185 countries},
    issn = {1078-8956, 1546-170X},
    url = {https://www.nature.com/articles/s41591-025-03502-3},
    doi = {10.1038/s41591-025-03502-3},
    language = {en},
    urldate = {2025-03-08},
    journal = {Nature Medicine},
    author = {Kim, Joanne and Harper, Andrew and McCormack, Valerie and Sung, Hyuna and Houssami, Nehmat and Morgan, Eileen and Mutebi, Miriam and Garvey, Gail and Soerjomataram, Isabelle and Fidler-Benaoudia, Miranda M.},
    month = feb,
    year = {2025},
}

@article{harnessing_2024,
author = {Wang, Yun and Bu, Na and Luan, Xiao-fei and Song, Qian-qian and Ma, Ba-Fang and Hao, Wenhui and Yan, Jing-jing and Wang, Li and Zheng, Xiao-ling and Maimaitiyiming, Yasen},
year = {2024},
month = {03},
pages = {},
title = {Harnessing the potential of long non-coding RNAs in breast cancer: from etiology to treatment resistance and clinical applications},
volume = {14},
journal = {Frontiers in Oncology},
doi = {10.3389/fonc.2024.1337579}
}

@misc{iarc2025press,
  author       = {International Agency for Research on Cancer},
  title        = {Breast cancer cases and deaths are projected to rise globally},
  howpublished = {Press Release No. 361, 24 February 2025},
  year         = {2025},
  institution  = {IARC},
  address      = {Lyon, France},
  url          = {https://www.iarc.who.int/wp-content/uploads/2025/02/pr361_E.pdf}
}
@article{wang_early_2017,
    title = {Early {Diagnosis} of {Breast} {Cancer}},
    volume = {17},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {1424-8220},
    url = {https://www.mdpi.com/1424-8220/17/7/1572},
    doi = {10.3390/s17071572},
    abstract = {Early-stage cancer detection could reduce breast cancer death rates significantly in the long-term. The most critical point for best prognosis is to identify early-stage cancer cells. Investigators have studied many breast diagnostic approaches, including mammography, magnetic resonance imaging, ultrasound, computerized tomography, positron emission tomography and biopsy. However, these techniques have some limitations such as being expensive, time consuming and not suitable for young women. Developing a high-sensitive and rapid early-stage breast cancer diagnostic method is urgent. In recent years, investigators have paid their attention in the development of biosensors to detect breast cancer using different biomarkers. Apart from biosensors and biomarkers, microwave imaging techniques have also been intensely studied as a promising diagnostic tool for rapid and cost-effective early-stage breast cancer detection. This paper aims to provide an overview on recent important achievements in breast screening methods (particularly on microwave imaging) and breast biomarkers along with biosensors for rapidly diagnosing breast cancer.},
    language = {en},
    number = {7},
    urldate = {2025-05-15},
    journal = {Sensors},
    author = {Wang, Lulu},
    month = jul,
    year = {2017},
    note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
    keywords = {biomarker, breast cancer, microwave biosensor, microwave imaging, microwave-sensing, radio frequency biosensor},
    pages = {1572},
}

@article{pattanaik_breast_2022,
    title = {Breast {Cancer} {Classification} from {Mammogram} {Images} {Using} {Extreme} {Learning} {Machine}-{Based} {DenseNet121} {Model}},
    volume = {2022},
    copyright = {Copyright © 2022 Raj Kumar Pattanaik et al.},
    issn = {1687-7268},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/2731364},
    doi = {10.1155/2022/2731364},
    abstract = {Breast cancer is characterized by abnormal discontinuities in the lining cells of a woman’s milk duct. Large numbers of women die from breast cancer as a result of developing symptoms in the milk ducts. If the diagnosis is made early, the death rates can be decreased. For radiologists and physicians, manually analyzing mammography images for breast cancer become time-consuming. To prevent manual analysis and simplify the work of classification, this paper introduces a novel hybrid DenseNet121-based Extreme Learning Machine Model (ELM) for classifying breast cancer from mammogram images. The mammogram images were processed through preprocessing and data augmentation phase. The features were collected separately after the pooling and flatten layer at the first stage of the classification. Further, the features are fed as input to the proposed DenseNet121-ELM model’s fully connected layer as input. An extreme learning machine model has replaced the fully connected layer. The weights of the extreme learning machine have been updated by the AdaGrad optimization algorithm to increase the model’s robustness and performance. Due to its faster convergence speed than other optimization techniques, the AdaGrad algorithm optimization was chosen. In this research, the Digital Database for Screening Mammography (DDSM) dataset mammogram images were utilized, and the results are presented. We have considered the batch size of 32, 64, and 128 for the performance measure, accuracy, sensitivity, specificity, and computational time. The proposed DenseNet121+ELM model achieves 99.47\% and 99.14\% as training accuracy and testing accuracy for batch size 128. Also, it achieves specificity, sensitivity, and computational time of 99.37\%, 99.94\%, and 159.7731 minutes, respectively. Further, the comparison result of performance measures is presented for batch sizes 32, 64, and 128 to show the robustness of the proposed DenseNet121+ELM model. The automatic classification performance of the DenseNet121+ELM model has much potential to be applied to the clinical diagnosis of breast cancer.},
    language = {en},
    number = {1},
    urldate = {2025-05-15},
    journal = {Journal of Sensors},
    author = {Pattanaik, Raj Kumar and Mishra, Satyasis and Siddique, Mohammed and Gopikrishna, Tiruveedula and Satapathy, Sunita},
    year = {2022},
    note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/2731364},
    pages = {2731364},
}
@article{meenalochini_deep_2024,
    title = {A {Deep} {Learning} {Based} {Breast} {Cancer} {Classification} {System} {Using} {Mammograms}},
    volume = {19},
    issn = {2093-7423},
    url = {https://doi.org/10.1007/s42835-023-01747-x},
    doi = {10.1007/s42835-023-01747-x},
    abstract = {An automatic breast cancer detection and classification system plays an essential role in medical imaging applications. But accurate disease identification is one of the complicated processes due to the existence of noisy contents and irrelevant structure of the original images. In conventional works, various medical image processing techniques have been developed for accurately classifying the types of breast cancer. Still, it confronts difficulties due to the aspects of increased complexity in computations, error values, false positives, and misclassification outputs. Hence, this research work proposes to develop an optimization-based classification system for the breast cancer identification system. Here, the Gaussian filtering and Adaptive Histogram Equalization (AHE) techniques are utilized for preprocessing the original mammogram images by eliminating the noisy contents and enhancing the contrast of an image. Then, the Markov Random Adaptive Segmentation (MRAS) technique is employed for detecting the boundary region based on the random value selection. To make the classifying procedure easier, the set of features is optimally extracted from the segmented region with the help of a Genetic Algorithm (GA). In which, the global best fitness value is estimated by using the crossover, mutation, and selection operations. Finally, the Convolutional Neural Network (CNN) classification technique is utilized for categorizing the image as to whether normal or abnormal with its type. The entire performance analysis of the suggested model is validated and compared using multiple measures during the evaluation. In the proposed method GA performs feature selection and prunes unnecessary features. The major goal is to improve the classification performance while reducing the number of features used. The proposed system GA-CNN provides improved performance results with a reduced error rate.The suggested GA-CNN increases accuracy (98.5), sensitivity (99.38), and specificity values (98.4) as compared to the existing technique by effectively identifying the classed label.},
    language = {en},
    number = {4},
    urldate = {2025-05-15},
    journal = {Journal of Electrical Engineering \& Technology},
    author = {Meenalochini, G. and Ramkumar, S.},
    month = may,
    year = {2024},
    keywords = {Automated Pattern Recognition, Breast cancer detection, Categorization, Computer Imaging, Vision, Pattern Recognition and Graphics, Contrast enhancement, Convolutional neural network (CNN) based classification, Genetic algorithm, Genetic algorithm (GA) based optimization, Image Processing, Learning algorithms, Machine Learning, Markov random adaptive segmentation (MRAS), Preprocessing},
    pages = {2637--2650},
}
@article{zahoor_breast_2022,
    title = {Breast {Cancer} {Mammograms} {Classification} {Using} {Deep} {Neural} {Network} and {Entropy}-{Controlled} {Whale} {Optimization} {Algorithm}},
    volume = {12},
    issn = {2075-4418},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8871265/},
    doi = {10.3390/diagnostics12020557},
    abstract = {Breast cancer has affected many women worldwide. To perform detection and classification of breast cancer many computer-aided diagnosis (CAD) systems have been established because the inspection of the mammogram images by the radiologist is a difficult and time taken task. To early diagnose the disease and provide better treatment lot of CAD systems were established. There is still a need to improve existing CAD systems by incorporating new methods and technologies in order to provide more precise results. This paper aims to investigate ways to prevent the disease as well as to provide new methods of classification in order to reduce the risk of breast cancer in women’s lives. The best feature optimization is performed to classify the results accurately. The CAD system’s accuracy improved by reducing the false-positive rates.The Modified Entropy Whale Optimization Algorithm (MEWOA) is proposed based on fusion for deep feature extraction and perform the classification. In the proposed method, the fine-tuned MobilenetV2 and Nasnet Mobile are applied for simulation. The features are extracted, and optimization is performed. The optimized features are fused and optimized by using MEWOA. Finally, by using the optimized deep features, the machine learning classifiers are applied to classify the breast cancer images. To extract the features and perform the classification, three publicly available datasets are used: INbreast, MIAS, and CBIS-DDSM. The maximum accuracy achieved in INbreast dataset is 99.7\%, MIAS dataset has 99.8\% and CBIS-DDSM has 93.8\%. Finally, a comparison with other existing methods is performed, demonstrating that the proposed algorithm outperforms the other approaches.},
    number = {2},
    urldate = {2025-05-15},
    journal = {Diagnostics},
    author = {Zahoor, Saliha and Shoaib, Umar and Lali, Ikram Ullah},
    month = feb,
    year = {2022},
    pmid = {35204646},
    pmcid = {PMC8871265},
    pages = {557},
}
@article{mota_breast_2024,
    title = {Breast {Cancer} {Molecular} {Subtype} {Prediction}: {A} {Mammography}-{Based} {AI} {Approach}},
    volume = {12},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {2227-9059},
    shorttitle = {Breast {Cancer} {Molecular} {Subtype} {Prediction}},
    url = {https://www.mdpi.com/2227-9059/12/6/1371},
    doi = {10.3390/biomedicines12061371},
    abstract = {Breast cancer remains a leading cause of mortality among women, with molecular subtypes significantly influencing prognosis and treatment strategies. Currently, identifying the molecular subtype of cancer requires a biopsy—a specialized, expensive, and time-consuming procedure, often yielding to results that must be supported with additional biopsies due to technique errors or tumor heterogeneity. This study introduces a novel approach for predicting breast cancer molecular subtypes using mammography images and advanced artificial intelligence (AI) methodologies. Using the OPTIMAM imaging database, 1397 images from 660 patients were selected. The pretrained deep learning model ResNet-101 was employed to classify tumors into five subtypes: Luminal A, Luminal B1, Luminal B2, HER2, and Triple Negative. Various classification strategies were studied: binary classifications (one vs. all others, specific combinations) and multi-class classification (evaluating all subtypes simultaneously). To address imbalanced data, strategies like oversampling, undersampling, and data augmentation were explored. Performance was evaluated using accuracy and area under the receiver operating characteristic curve (AUC). Binary classification results showed a maximum average accuracy and AUC of 79.02\% and 64.69\%, respectively, while multi-class classification achieved an average AUC of 60.62\% with oversampling and data augmentation. The most notable binary classification was HER2 vs. non-HER2, with an accuracy of 89.79\% and an AUC of 73.31\%. Binary classification for specific combinations of subtypes revealed an accuracy of 76.42\% for HER2 vs. Luminal A and an AUC of 73.04\% for HER2 vs. Luminal B1. These findings highlight the potential of mammography-based AI for non-invasive breast cancer subtype prediction, offering a promising alternative to biopsies and paving the way for personalized treatment plans.},
    language = {en},
    number = {6},
    urldate = {2025-05-15},
    journal = {Biomedicines},
    author = {Mota, Ana M. and Mendes, João and Matela, Nuno},
    month = jun,
    year = {2024},
    note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
    keywords = {artificial intelligence, breast cancer, deep learning, mammography, molecular subtypes, personalized medicine},
    pages = {1371},
}
@article{ben_rabah_multimodal_2025,
    title = {A {Multimodal} {Deep} {Learning} {Model} for the {Classification} of {Breast} {Cancer} {Subtypes}},
    volume = {15},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {2075-4418},
    url = {https://www.mdpi.com/2075-4418/15/8/995},
    doi = {10.3390/diagnostics15080995},
    abstract = {Background: Breast cancer is a heterogeneous disease with distinct molecular subtypes, each requiring tailored therapeutic strategies. Accurate classification of these subtypes is crucial for optimizing treatment and improving patient outcomes. While immunohistochemistry remains the gold standard for subtyping, it is invasive and may not fully capture tumor heterogeneity. Artificial Intelligence (AI), particularly Deep Learning (DL), offers a promising non-invasive alternative by analyzing medical imaging data. Methods: In this study, we propose a multimodal DL model that integrates mammography images with clinical metadata to classify breast lesions into five categories: benign, luminal A, luminal B, HER2-enriched, and triple-negative. Using the publicly available Chinese Mammography Database (CMMD), our model was trained and evaluated on a dataset of 4056 images from 1775 patients. Results: The proposed multimodal approach significantly outperformed a unimodal model based solely on mammography images, achieving an AUC of 88.87\% for multiclass classification of these five categories, compared to 61.3\% AUC for the unimodal model. Conclusions: These findings highlight the potential of multimodal AI-driven approaches for non-invasive breast cancer subtype classification, paving the way for improved diagnostic precision and personalized treatment strategies.},
    language = {en},
    number = {8},
    urldate = {2025-04-18},
    journal = {Diagnostics},
    author = {Ben Rabah, Chaima and Sattar, Aamenah and Ibrahim, Ahmed and Serag, Ahmed},
    month = jan,
    year = {2025},
    note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
    keywords = {artificial intelligence, breast cancer, deep learning, molecular subtype classification, multimodal, personalized medicine},
    pages = {995},
}
@article{cai_online_2023,
    title = {An {Online} {Mammography} {Database} with {Biopsy} {Confirmed} {Types}},
    volume = {10},
    copyright = {2023 The Author(s)},
    issn = {2052-4463},
    url = {https://www.nature.com/articles/s41597-023-02025-1},
    doi = {10.1038/s41597-023-02025-1},
    abstract = {Breast carcinoma is the second largest cancer in the world among women. Early detection of breast cancer has been shown to increase the survival rate, thereby significantly increasing patients’ lifespan. Mammography, a noninvasive imaging tool with low cost, is widely used to diagnose breast disease at an early stage due to its high sensitivity. Although some public mammography datasets are useful, there is still a lack of open access datasets that expand beyond the white population as well as missing biopsy confirmation or with unknown molecular subtypes. To fill this gap, we build a database containing two online breast mammographies. The dataset named by Chinese Mammography Database (CMMD) contains 3712 mammographies involved 1775 patients, which is divided into two branches. The first dataset CMMD1 contains 1026 cases (2214 mammographies) with biopsy confirmed type of benign or malignant tumors. The second dataset CMMD2 includes 1498 mammographies for 749 patients with known molecular subtypes. Our database is constructed to enrich the diversity of mammography data and promote the development of relevant fields.},
    language = {en},
    number = {1},
    urldate = {2025-05-18},
    journal = {Scientific Data},
    author = {Cai, Hongmin and Wang, Jinhua and Dan, Tingting and Li, Jiao and Fan, Zhihao and Yi, Weiting and Cui, Chunyan and Jiang, Xinhua and Li, Li},
    month = mar,
    year = {2023},
    note = {Publisher: Nature Publishing Group},
    keywords = {Breast cancer, Prognostic markers},
    pages = {123},
}
@incollection{hussain_performance_2025,
    title = {Performance {Evaluation} of {Deep} {Learning} and {Transformer} {Models} {Using} {Multimodal} {Data} for {Breast} {Cancer} {Classification}},
    volume = {15199},
    url = {http://arxiv.org/abs/2410.10146},
    abstract = {Rising breast cancer (BC) occurrence and mortality are major global concerns for women. Deep learning (DL) has demonstrated superior diagnostic performance in BC classification compared to human expert readers. However, the predominant use of unimodal (digital mammography) features may limit the current performance of diagnostic models. To address this, we collected a novel multimodal dataset comprising both imaging and textual data. This study proposes a multimodal DL architecture for BC classification, utilising images (mammograms; four views) and textual data (radiological reports) from our new in-house dataset. Various augmentation techniques were applied to enhance the training data size for both imaging and textual data. We explored the performance of eleven SOTA DL architectures (VGG16, VGG19, ResNet34, ResNet50, MobileNet-v3, EffNet-b0, EffNet-b1, EffNet-b2, EffNet-b3, EffNet-b7, and Vision Transformer (ViT)) as imaging feature extractors. For textual feature extraction, we utilised either artificial neural networks (ANNs) or long short-term memory (LSTM) networks. The combined imaging and textual features were then inputted into an ANN classifier for BC classification, using the late fusion technique. We evaluated different feature extractor and classifier arrangements. The VGG19 and ANN combinations achieved the highest accuracy of 0.951. For precision, the VGG19 and ANN combination again surpassed other CNN and LSTM, ANN based architectures by achieving a score of 0.95. The best sensitivity score of 0.903 was achieved by the VGG16+LSTM. The highest F1 score of 0.931 was achieved by VGG19+LSTM. Only the VGG16+LSTM achieved the best area under the curve (AUC) of 0.937, with VGG16+LSTM closely following with a 0.929 AUC score.},
    urldate = {2025-05-19},
    author = {Hussain, Sadam and Ali, Mansoor and Naseem, Usman and Palomo, Beatriz Alejandra Bosques and Molina, Mario Alexis Monsivais and Abdala, Jorge Alberto Garza and Avalos, Daly Betzabeth Avendano and Cardona-Huerta, Servando and Gulliver, T. Aaron and Pena, Jose Gerardo Tamez},
    year = {2025},
    doi = {10.1007/978-3-031-73376-5_6},
    note = {arXiv:2410.10146 [eess]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
    pages = {59--69},
}
@article{mckinney_international_2020,
    title = {International evaluation of an {AI} system for breast cancer screening},
    volume = {577},
    copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
    issn = {1476-4687},
    url = {https://www.nature.com/articles/s41586-019-1799-6},
    doi = {10.1038/s41586-019-1799-6},
    abstract = {Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7\% and 1.2\% (USA and UK) in false positives and 9.4\% and 2.7\% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5\%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88\%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.},
    language = {en},
    number = {7788},
    urldate = {2025-05-19},
    journal = {Nature},
    author = {McKinney, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg S. and Darzi, Ara and Etemadi, Mozziyar and Garcia-Vicente, Florencia and Gilbert, Fiona J. and Halling-Brown, Mark and Hassabis, Demis and Jansen, Sunny and Karthikesalingam, Alan and Kelly, Christopher J. and King, Dominic and Ledsam, Joseph R. and Melnick, David and Mostofi, Hormuz and Peng, Lily and Reicher, Joshua Jay and Romera-Paredes, Bernardino and Sidebottom, Richard and Suleyman, Mustafa and Tse, Daniel and Young, Kenneth C. and De Fauw, Jeffrey and Shetty, Shravya},
    month = jan,
    year = {2020},
    note = {Publisher: Nature Publishing Group},
    keywords = {Breast cancer, Preclinical research},
    pages = {89--94},
}
@article{rashmi_predicting_2021,
    title = {Predicting the molecular subtype of breast cancer based on mammography and ultrasound findings},
    volume = {28},
    copyright = {Thieme Medical and Scientific Publishers Pvt. Ltd. A-12, 2nd Floor, Sector 2, Noida-201301 UP, India},
    issn = {0971-3026},
    url = {https://www.thieme-connect.de/products/ejournals/abstract/10.4103/ijri.IJRI_78_18},
    doi = {10.4103/ijri.IJRI_78_18},
    abstract = {Aim: To determine the correlation between mammography and ultrasound features of breast cancer with molecular subtypes and to calculate the predictive value of these features. Materials and Method: This is a prospective study of consecutive patients with breast cancer presenting between January 2016 and July 2017, who underwent mammography and/or ultrasound of breast and excision of the breast mass. Patients with contralateral breast mass, metastases, h/o prior cancer treatment, and other malignancies were excluded. On mammography, the presence or absence of microcalcification was noted. On ultrasound examination size, margins, microcalcification, posterior acoustic features, vascularity, and axillary nodes were assessed. Margins were categorized into circumscribed and non-circumscribed. Posterior acoustic features were classified into four categories: shadowing, enhancement, mixed, and no changes. Vascularity was assessed based on Adler's index into grades 0, 1, 2, and 3. Grades 0 and 1 were considered low and 2 and 3 were high. Results: Tumors with non-circumscribed margins and posterior acoustic shadowing were likely to be luminal A or B subtype of breast cancer [odds ratio (JR) 5.78; 95\% confidence interval (CI) 3.68–9.80; P {\textless} 0.0001]. Tumors with non-circumscribed margins, posterior acoustic shadowing, and high vascularity were more likely to be luminal B subtype (JR 2.88; 95\% CI 2–4.14; P- {\textless}0.0001). Tumors with microcalcification and posterior mixed acoustic pattern were strongly associated to be HER2-positive (JR 5.48; 95\% CI 3.06–9.80; P {\textless} 0.0001). Tumors with circumscribed margins and posterior acoustic enhancement were highly suggestive of triple-negative breast cancer (JR 7.06; 95\% CI 4.64–10.73; P {\textless} 0.0001). Conclusion: Microcalcification detected on mammography and certain ultrasound features such as circumscribed or non-circumscribed margins, posterior acoustic features, and vascularity are strongly correlated in predicting the molecular subtypes of breast cancer, and thus may further expand the role of conventional breast imaging.},
    language = {en},
    urldate = {2025-05-19},
    journal = {Indian Journal of Radiology and Imaging},
    author = {Rashmi, S. and Kamala, S. and Murthy, S. Sudha and Kotha, Swapna and Rao, Y. Suhas and Chaudhary, K. Veeraiah},
    month = jul,
    year = {2021},
    note = {Publisher: Thieme Medical and Scientific Publishers Pvt. Ltd.},
    pages = {354--361},
}
@article{goldhirsch_personalizing_2013,
    title = {Personalizing the treatment of women with early breast cancer: highlights of the {St} {Gallen} {International} {Expert} {Consensus} on the {Primary} {Therapy} of {Early} {Breast} {Cancer} 2013},
    volume = {24},
    issn = {0923-7534, 1569-8041},
    shorttitle = {Personalizing the treatment of women with early breast cancer},
    url = {https://www.annalsofoncology.org/article/S0923-7534(19)36964-9/fulltext},
    doi = {10.1093/annonc/mdt303},
    language = {English},
    number = {9},
    urldate = {2025-05-24},
    journal = {Annals of Oncology},
    author = {Goldhirsch, A. and Winer, E. P. and Coates, A. S. and Gelber, R. D. and Piccart-Gebhart, M. and Thürlimann, B. and Senn, H.-J. and Albain, Kathy S. and André, Fabrice and Bergh, Jonas and Bonnefoi, Hervé and Bretel-Morales, Denisse and Burstein, Harold and Cardoso, Fatima and Castiglione-Gertsch, Monica and Coates, Alan S. and Colleoni, Marco and Costa, Alberto and Curigliano, Giuseppe and Davidson, Nancy E. and Leo, Angelo Di and Ejlertsen, Bent and Forbes, John F. and Gelber, Richard D. and Gnant, Michael and Goldhirsch, Aron and Goodwin, Pamela and Goss, Paul E. and Harris, Jay R. and Hayes, Daniel F. and Hudis, Clifford A. and Ingle, James N. and Jassem, Jacek and Jiang, Zefei and Karlsson, Per and Loibl, Sibylle and Morrow, Monica and Namer, Moise and Osborne, C. Kent and Partridge, Ann H. and Penault-Llorca, Frédérique and Perou, Charles M. and Piccart-Gebhart, Martine J. and Pritchard, Kathleen I. and Rutgers, Emiel J. T. and Sedlmayer, Felix and Semiglazov, Vladimir and Shao, Zhi-Ming and Smith, Ian and Thürlimann, Beat and Toi, Masakazu and Tutt, Andrew and Untch, Michael and Viale, Giuseppe and Watanabe, Toru and Wilcken, Nicholas and Winer, Eric P. and Wood, William C.},
    month = sep,
    year = {2013},
    pmid = {23917950},
    note = {Publisher: Elsevier},
    keywords = {St Gallen Consensus, early breast cancer, radiation therapy, subtypes, surgery, systemic adjuvant therapies},
    pages = {2206--2223},
}
@article{mauricio_comparing_2023,
    title = {Comparing {Vision} {Transformers} and {Convolutional} {Neural} {Networks} for {Image} {Classification}: {A} {Literature} {Review}},
    volume = {13},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {2076-3417},
    shorttitle = {Comparing {Vision} {Transformers} and {Convolutional} {Neural} {Networks} for {Image} {Classification}},
    url = {https://www.mdpi.com/2076-3417/13/9/5521},
    doi = {10.3390/app13095521},
    abstract = {Transformers are models that implement a mechanism of self-attention, individually weighting the importance of each part of the input data. Their use in image classification tasks is still somewhat limited since researchers have so far chosen Convolutional Neural Networks for image classification and transformers were more targeted to Natural Language Processing (NLP) tasks. Therefore, this paper presents a literature review that shows the differences between Vision Transformers (ViT) and Convolutional Neural Networks. The state of the art that used the two architectures for image classification was reviewed and an attempt was made to understand what factors may influence the performance of the two deep learning architectures based on the datasets used, image size, number of target classes (for the classification problems), hardware, and evaluated architectures and top results. The objective of this work is to identify which of the architectures is the best for image classification and under what conditions. This paper also describes the importance of the Multi-Head Attention mechanism for improving the performance of ViT in image classification.},
    language = {en},
    number = {9},
    urldate = {2025-05-25},
    journal = {Applied Sciences},
    author = {Maurício, José and Domingues, Inês and Bernardino, Jorge},
    month = jan,
    year = {2023},
    note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
    keywords = {Vision Transformers (ViT), convolutional neural networks, image classification, multi-head attention, transformers},
    pages = {5521},
}
@misc{who_breast_2024,
    title = {Breast cancer},
    url = {https://www.who.int/news-room/fact-sheets/detail/breast-cancer},
    language = {en},
    urldate = {2025-05-28},
    year = {2024},
}
@misc{seom_cancer_nodate,
    title = {El cáncer en cifras},
    url = {https://seom.org/prensa/el-cancer-en-cifras},
    urldate = {2025-05-28},
}
@misc{cleveland_clinic_breast_2023,
    title = {Breast {Cancer}},
    url = {https://my.clevelandclinic.org/health/diseases/3986-breast-cancer},
    abstract = {Thanks to breast cancer awareness, research and new treatments, more people are living with breast cancer. Read on to find out more.},
    language = {en},
    urldate = {2025-05-28},
    journal = {Cleveland Clinic},
    author = {{Cleveland Clinic}},
    year = {2023},
}
@misc{noauthor_nci_2011,
    type = {{nciAppModulePage}},
    title = {{NCI} {Dictionary} of {Cancer} {Terms} - {NCI}},
    url = {https://www.cancer.gov/publications/dictionaries/cancer-terms/},
    abstract = {NCI's Dictionary of Cancer Terms provides easy-to-understand definitions for words and phrases related to cancer and medicine.},
    language = {en},
    urldate = {2025-05-28},
    month = feb,
    year = {2011},
    note = {Archive Location: nciglobal,ncienterprise},
}
@article{perou_molecular_2000,
    title = {Molecular portraits of human breast tumours},
    volume = {406},
    issn = {0028-0836},
    doi = {10.1038/35021093},
    abstract = {Human breast tumours are diverse in their natural history and in their responsiveness to treatments. Variation in transcriptional programs accounts for much of the biological diversity of human cells and tumours. In each cell, signal transduction and regulatory systems transduce information from the cell's identity to its environmental status, thereby controlling the level of expression of every gene in the genome. Here we have characterized variation in gene expression patterns in a set of 65 surgical specimens of human breast tumours from 42 different individuals, using complementary DNA microarrays representing 8,102 human genes. These patterns provided a distinctive molecular portrait of each tumour. Twenty of the tumours were sampled twice, before and after a 16-week course of doxorubicin chemotherapy, and two tumours were paired with a lymph node metastasis from the same patient. Gene expression patterns in two tumour samples from the same individual were almost always more similar to each other than either was to any other sample. Sets of co-expressed genes were identified for which variation in messenger RNA levels could be related to specific features of physiological variation. The tumours could be classified into subtypes distinguished by pervasive differences in their gene expression patterns.},
    language = {eng},
    number = {6797},
    journal = {Nature},
    author = {Perou, C. M. and Sørlie, T. and Eisen, M. B. and van de Rijn, M. and Jeffrey, S. S. and Rees, C. A. and Pollack, J. R. and Ross, D. T. and Johnsen, H. and Akslen, L. A. and Fluge, O. and Pergamenschikov, A. and Williams, C. and Zhu, S. X. and Lønning, P. E. and Børresen-Dale, A. L. and Brown, P. O. and Botstein, D.},
    month = aug,
    year = {2000},
    pmid = {10963602},
    keywords = {Breast Neoplasms, DNA, Neoplasm, Female, Gene Expression, Gene Expression Profiling, Genes, erbB-2, Humans, Oligonucleotide Array Sequence Analysis, Phenotype, Tumor Cells, Cultured},
    pages = {747--752},
}
@article{sorlie_repeated_2003,
    title = {Repeated observation of breast tumor subtypes in independent gene expression data sets},
    volume = {100},
    url = {https://www.pnas.org/doi/10.1073/pnas.0932692100},
    doi = {10.1073/pnas.0932692100},
    abstract = {Characteristic patterns of gene expression measured by DNA microarrays have been used to classify tumors into clinically relevant subgroups. In this study, we have refined the previously defined subtypes of breast tumors that could be distinguished by their distinct patterns of gene expression. A total of 115 malignant breast tumors were analyzed by hierarchical clustering based on patterns of expression of 534 “intrinsic” genes and shown to subdivide into one basal-like, one ERBB2-overexpressing, two luminal-like, and one normal breast tissue-like subgroup. The genes used for classification were selected based on their similar expression levels between pairs of consecutive samples taken from the same tumor separated by 15 weeks of neoadjuvant treatment. Similar cluster analyses of two published, independent data sets representing different patient cohorts from different laboratories, uncovered some of the same breast cancer subtypes. In the one data set that included information on time to development of distant metastasis, subtypes were associated with significant differences in this clinical feature. By including a group of tumors from BRCA1 carriers in the analysis, we found that this genotype predisposes to the basal tumor subtype. Our results strongly support the idea that many of these breast tumor subtypes represent biologically distinct disease entities.},
    number = {14},
    urldate = {2025-05-29},
    journal = {Proceedings of the National Academy of Sciences},
    author = {Sørlie, Therese and Tibshirani, Robert and Parker, Joel and Hastie, Trevor and Marron, J. S. and Nobel, Andrew and Deng, Shibing and Johnsen, Hilde and Pesich, Robert and Geisler, Stephanie and Demeter, Janos and Perou, Charles M. and Lønning, Per E. and Brown, Patrick O. and Børresen-Dale, Anne-Lise and Botstein, David},
    month = jul,
    year = {2003},
    note = {Publisher: Proceedings of the National Academy of Sciences},
    pages = {8418--8423},
}
@article{lips_breast_2013,
    title = {Breast cancer subtyping by immunohistochemistry and histological grade outperforms breast cancer intrinsic subtypes in predicting neoadjuvant chemotherapy response},
    volume = {140},
    issn = {0167-6806},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3706735/},
    doi = {10.1007/s10549-013-2620-0},
    abstract = {Intrinsic subtypes are widely accepted for the classification of breast cancer. Lacking gene expression data, surrogate classifications based on immunohistochemistry (IHC) have been proposed. A recent St. Gallen consensus meeting recommends to use this “surrogate intrinsic subtypes” for predicting adjuvant chemotherapy resistance, implying that “Surrogate Luminal A” breast cancers should only receive endocrine therapy. In this study we assessed both gene expression based intrinsic subtypes as well as surrogate intrinsic subtypes regarding their power to predict neoadjuvant chemotherapy benefit. Single institution data of 560 breast cancer patients were reviewed. Gene expression data was available for 247 patients. Subtypes were determined on the basis of IHC, Ki67, histological grade, endocrine responsiveness, and gene expression, and were correlated with chemotherapy response and recurrence-free survival. In ER+/HER2− tumors, a high histological grade was the best predictor for chemotherapy benefit, both in terms of pCR (p = 0.004) and recurrence-free survival (p = 0.002). The gene expression based and surrogate intrinsic subtype based on Ki67 had no predictive or prognostic value in ER+/HER2− tumors. Histological grade, ER, PR, and HER2 were the best predictive factors for chemotherapy response in breast cancer. We propose to continue the conventional use of these markers.},
    number = {1},
    urldate = {2025-05-29},
    journal = {Breast Cancer Research and Treatment},
    author = {Lips, E. H. and Mulder, L. and de Ronde, J. J. and Mandjes, I. A. M. and Koolen, B. B. and Wessels, L. F. A. and Rodenhuis, S. and Wesseling, J.},
    year = {2013},
    pmid = {23828499},
    pmcid = {PMC3706735},
    pages = {63--71},
}
@misc{noauthor_bi-rads_2025,
    title = {{BI}-{RADS} {Score}: {Understanding} {Breast} {Imaging} {Results}},
    shorttitle = {{BI}-{RADS} {Score}},
    url = {https://www.healthcentral.com/article/birads-breast-imaging-reporting-and-data-system},
    abstract = {BI-RADS score is a system used to categorize mammogram findings, indicating the likelihood of breast cancer.},
    language = {en},
    urldate = {2025-06-01},
    journal = {HealthCentral},
    month = may,
    year = {2025},
}
@misc{noauthor_stages_nodate,
    title = {Stages of {Breast} {Cancer} {\textbar} {Understand} {Breast} {Cancer} {Staging}},
    url = {https://www.cancer.org/cancer/types/breast-cancer/understanding-a-breast-cancer-diagnosis/stages-of-breast-cancer.html},
    abstract = {When someone is diagnosed with breast cancer, doctors will determine if it has spread. This process is called staging. Learn about what your cancer stage means.},
    language = {en},
    urldate = {2025-06-01},
}
@article{hortobagyi_new_2018,
    title = {New and {Important} {Changes} in the {TNM} {Staging} {System} for {Breast} {Cancer}},
    issn = {1548-8748},
    url = {https://ascopubs.org/doi/10.1200/EDBK_201313},
    doi = {10.1200/EDBK_201313},
    number = {38},
    urldate = {2025-06-01},
    journal = {American Society of Clinical Oncology Educational Book},
    author = {Hortobagyi, Gabriel N. and Edge, Stephen B. and Giuliano, Armando},
    month = may,
    year = {2018},
    note = {Publisher: Wolters Kluwer},
    pages = {457--467},
}
@misc{kashiwada_tompei-cmmd_2025,
    title = {{TOMPEI}-{CMMD} {Dataset}},
    url = {https://doi.org/10.7937/WEZW-BH22},
    doi = {10.7937/WEZW-BH22},
    publisher = {The Cancer Imaging Archive},
    author = {Kashiwada, Y and Takaya, E and Hiroya, M and Matsuda, N and Yashima, T and Kobayashi, T and Tamiya, G and Ueda, T},
    year = {2025},
}
@article{makki_diversity_2015,
    title = {Diversity of {Breast} {Carcinoma}: {Histological} {Subtypes} and {Clinical} {Relevance}},
    volume = {8},
    issn = {1179-5557},
    shorttitle = {Diversity of {Breast} {Carcinoma}},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4689326/},
    doi = {10.4137/CPath.S31563},
    abstract = {Mammary carcinoma is the most common malignant tumor in women, and it is the leading cause of mortality, with an incidence of {\textgreater}1,000,000 cases occurring worldwide annually. It is one of the most common human neoplasms, accounting for approximately one-quarter of all cancers in females worldwide and 27\% of cancers in developed countries with a Western lifestyle. They exhibit a wide scope of morphological features, different immunohistochemical profiles, and unique histopathological subtypes that have specific clinical course and outcome. Breast cancers can be classified into distinct subgroups based on similarities in the gene expression profiles and molecular classification.},
    urldate = {2025-06-03},
    journal = {Clinical Medicine Insights. Pathology},
    author = {Makki, Jaafar},
    month = dec,
    year = {2015},
    pmid = {26740749},
    pmcid = {PMC4689326},
    pages = {23--31},
}
@misc{noauthor_types_nodate,
    title = {Types of {Breast} {Cancer} {\textbar} {About} {Breast} {Cancer}},
    url = {https://www.cancer.org/cancer/types/breast-cancer/about/types-of-breast-cancer.html},
    abstract = {Learn about the different types of breast cancer including the common types of DCIS, invasive ductal carcinoma, and invasive lobular carcinoma.},
    language = {en},
    urldate = {2025-06-03},
}
@incollection{magny_breast_2025,
    address = {Treasure Island (FL)},
    title = {Breast {Imaging} {Reporting} and {Data} {System}},
    copyright = {Copyright © 2025, StatPearls Publishing LLC.},
    url = {http://www.ncbi.nlm.nih.gov/books/NBK459169/},
    abstract = {Breast imaging-reporting and data system (BI-RADS) is a classification system proposed by the American College of Radiology (ACR) in 1986 with the original report released in 1993. The 1980s saw an exponential increase in mammography with the implementation of yearly screening mammograms and overwhelming variation amongst radiology reports. BI-RADS was implemented to standardize risk assessment and quality control for mammography and provide uniformity in the reports for non-radiologist. The first version proposed included the suggested structure for a mammographic report, the lexicon for mammographic imaging findings, and final assessment category with recommendations for management. The ACR used scientific analysis and literature review to create a lexicon of descriptors that had shown to correlate with high predictive values associated with either benign or malignant disease. The second important aspect of the BI-RADS system was the category classification for the overall assessment of the imaging findings. The categorization provides an approximate risk of malignancy to a lesion from essentially zero to greater than 95\%. The categorization and final assessment decreased ambiguity in recommendations. BI-RADS was built to be fluid and change with the adaptation of new techniques and research. Such changes that have occurred are the inclusion of lexicons for ultrasound in 2003 and MRI in 2006. The latest edition is BI-RADS 5 (2013) and included six classifications for lesions.},
    language = {eng},
    urldate = {2025-06-03},
    booktitle = {{StatPearls}},
    publisher = {StatPearls Publishing},
    author = {Magny, Samuel J. and Shikhman, Rachel and Keppke, Ana L.},
    year = {2025},
    pmid = {29083600},
}
@misc{staff_what_2025,
    title = {What {You} {Need} to {Know} {About} {X}-{Ray} in {Mammography} - {Centers} {Urgent} {Care}},
    url = {https://centersurgentcare.net/x-ray-in-mammography/general/, https://centersurgentcare.net/x-ray-in-mammography/general/},
    abstract = {Early detection saves lives—see how x-rays in mammography reveal hidden breast changes long before symptoms appear.},
    language = {en-US},
    urldate = {2025-06-03},
    author = {Staff},
    month = may,
    year = {2025},
    note = {Section: General},
}

@article{iranmakani_review_2020,
    title = {A review of various modalities in breast imaging: technical aspects and clinical outcomes},
    volume = {51},
    issn = {2090-4762},
    shorttitle = {A review of various modalities in breast imaging},
    url = {https://doi.org/10.1186/s43055-020-00175-5},
    doi = {10.1186/s43055-020-00175-5},
    abstract = {Nowadays, breast cancer is the second cause of death after cardiovascular diseases. In general, about one out of eight women (about 12\%) suffer from this disease during their life in the USA and European countries. If breast cancer is detected at an early stage, its survival rate will be very high. Several methods have been introduced to diagnose breast cancer with their clinical advantages and disadvantages.},
    number = {1},
    urldate = {2025-06-01},
    journal = {Egyptian Journal of Radiology and Nuclear Medicine},
    author = {Iranmakani, Sepideh and Mortezazadeh, Tohid and Sajadian, Fakhrossadat and Ghaziani, Mona Fazel and Ghafari, Ali and Khezerloo, Davood and Musa, Ahmed Eleojo},
    month = apr,
    year = {2020},
    keywords = {Breast cancer, MRI, Mammography, Microwave imaging, Nuclear medicine, Optical imaging, Sonography},
    pages = {57},
}
@misc{ltd_mammography_2025,
    title = {Mammography {Market} {Size}, {Share} \& {Opportunities} 2025-2032},
    url = {https://www.coherentmarketinsights.com/market-insight/mammography-market-5425},
    abstract = {Mammography Market is estimated to be valued at USD 2.87 Bn in 2025 and is expected to expand at annual growth rate of 10.4\% , reaching USD 5.74 Bn by 2032.},
    language = {en},
    urldate = {2025-06-03},
    journal = {Coherent Market Insights},
    author = {Ltd, Coherent Market Insights Pvt},
    month = feb,
    year = {2025},
}
@misc{noauthor_mammography_nodate,
    title = {Mammography {Market} {Size} \& {Share} {\textbar} {Industry} {Report}, 2030},
    url = {https://www.grandviewresearch.com/industry-analysis/mammography-market},
    abstract = {The global mammography market size was estimated at USD 2.58 billion in 2024 and is projected to grow at a CAGR of 10.50\% from 2025 to 2030},
    language = {en},
    urldate = {2025-06-03},
}
@misc{noauthor_guide_nodate,
    title = {Guide to {Mammography} {Views} \& {Positioning} {\textbar} {MTMI}},
    url = {https://www.mtmi.net/blog/guide-mammography-views-positioning},
    abstract = {Proper mammographic positioning is essential to the detection and diagnosis of breast cancer. Without properly positioned and exposed images, the radiologist cannot clearly find the presence of abnormalities which would possibly require a biopsy. Skilled, dedicated mammographers must recognize the importance of their role and must always pay close attention to detail so they can produce high-quality, mammograms on behalf of both the patient and the radiologist. This requires ongoing education and a thorough understanding of normal breast anatomy.},
    language = {en},
    urldate = {2025-06-03},
}
@misc{aljarrah_trends_2014,
    title = {Trends in the distribution of breast cancer over time in the southeast of {Scotland} and review of the literature},
    copyright = {© 2019 the authors; licensee ecancermedicalscience. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
    url = {http://ecancer.org/en/journal/article/427-trends-in-the-distribution-of-breast-cancer-over-time-in-the-southeast-of-scotland-and-review-of-the-literature},
    abstract = {Trends in the distribution of breast cancer over time in the southeast of Scotland and review of the literature A Aljarrah1,2 and WR Miller2},
    language = {en},
    urldate = {2025-06-03},
    author = {Aljarrah, A. and Miller, W. R.},
    month = may,
    year = {2014},
    doi = {10.3332/ecancer.2014.427},
}
@misc{noauthor_breast_2015,
    title = {Breast cancer incidence (invasive) statistics},
    url = {https://www.cancerresearchuk.org/health-professional/cancer-statistics/statistics-by-cancer-type/breast-cancer/incidence-invasive},
    abstract = {The latest breast cancer incidence invasive statistics for the UK for Health Professionals. See data for age, trends over time, stage at diagnosis and more.},
    language = {en},
    urldate = {2025-06-03},
    journal = {Cancer Research UK},
    month = may,
    year = {2015},
}
@misc{imaging_introduction_2022,
    title = {Introduction to {Mammography}: {The} {Basics}},
    shorttitle = {Introduction to {Mammography}},
    url = {https://clinicalpub.com/introduction-to-mammography-the-basics/},
    abstract = {Overview This chapter is a basic introduction to the fundamentals of mammography, including standard and special views, technical adequacy, and normal anatomy . Breast imaging is integral to the diagnosis and evaluation of breast cancer and other breast pathologies. While ultrasound and magnetic resonance radiology (MRI) also play important roles, mammography is often considered the […]},
    language = {en-US},
    urldate = {2025-06-03},
    journal = {Clinical Tree},
    author = {Imaging, Breast},
    month = dec,
    year = {2022},
}
@article{gokhale_ultrasound_2009,
    title = {Ultrasound characterization of breast masses},
    volume = {19},
    issn = {0971-3026},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766883/},
    doi = {10.4103/0971-3026.54878},
    abstract = {A lump in the breast is a cause of great concern. High frequency, high-resolution USG helps in its evaluation. This is exemplified in women with dense breast tissue where USG is useful in detecting small breast cancers that are not seen on mammography. Several studies in the past have addressed the issue of differentiating benign from malignant lesions in the breast. The American College of Radiology has also brought out a BIRADS-US classification system for categorizing focal breast lesions.},
    number = {3},
    urldate = {2025-06-03},
    journal = {The Indian Journal of Radiology \& Imaging},
    author = {Gokhale, Sudheer},
    month = aug,
    year = {2009},
    pmid = {19881096},
    pmcid = {PMC2766883},
    pages = {242--247},
}
@misc{noauthor_breast_nodate,
    title = {Breast {Ultrasound} ({Sonogram})},
    url = {https://densebreast-info.org/screening-technologies/breast-ultrasound/},
    language = {en},
    urldate = {2025-06-03},
    journal = {DenseBreast-info, Inc.},
}
@misc{radswiki_breast_nodate,
    title = {Breast {MRI} {\textbar} {Radiology} {Reference} {Article} {\textbar} {Radiopaedia}.org},
    url = {https://radiopaedia.org/articles/breast-mri},
    abstract = {Breast MRI is the most sensitive method ({\textgreater}90\%) for the detection of breast cancer. Its role in diagnosis and management continues to evolve 13.
Terminology
Dynamic contrast-enhanced (DCE)-MRI provides information about the morphology and functi...},
    language = {en-US},
    urldate = {2025-06-03},
    journal = {Radiopaedia},
    author = {Radswiki, The},
    doi = {10.53347/rID-12182},
}
@misc{noauthor_technical_nodate,
    title = {Technical guidelines for {MRI} for the surveillance of women at higher risk of developing breast cancer},
    url = {https://www.gov.uk/government/publications/nhs-breast-screening-using-mri-with-higher-risk-women/technical-guidelines-for-mri-for-the-surveillance-of-women-at-higher-risk-of-developing-breast-cancer},
    language = {en},
    urldate = {2025-06-03},
    journal = {GOV.UK},
}

@misc{nih_definition_2011,
    type = {Definition of {MRI}},
    title = {Definition of {MRI} - {NCI} {Dictionary} of {Cancer} {Terms} - {NCI}},
    shorttitle = {Definition of {MRI}},
    url = {https://www.cancer.gov/publications/dictionaries/cancer-terms/def/mri},
    abstract = {NCI's Dictionary of Cancer Terms provides easy-to-understand definitions for words and phrases related to cancer and medicine.},
    language = {en},
    urldate = {2025-06-03},
    journal = {Definition of MRI},
    author = {{NIH}},
    month = feb,
    year = {2011},
    note = {Archive Location: nciglobal,ncienterprise},
}
@misc{nihDefinitionMammogramNCI2011,
    type = {{nciAppModulePage}},
    title = {Definition of mammogram - {NCI} {Dictionary} of {Cancer} {Terms} - {NCI}},
    shorttitle = {Definition of {Mammogram}},
    url = {https://www.cancer.gov/publications/dictionaries/cancer-terms/def/mammogram},
    abstract = {NCI's Dictionary of Cancer Terms provides easy-to-understand definitions for words and phrases related to cancer and medicine.},
    language = {en},
    urldate = {2025-06-03},
    journal = {Definition of Mammogram},
    author = {{NIH}},
    month = feb,
    year = {2011},
    note = {Archive Location: nciglobal,ncienterprise},
}
@misc{macauley_start--finish_2022,
    title = {A {Start}-{To}-{Finish} {Guide} {To} {Performing} {A} {Breast} {Ultrasound}},
    url = {https://sonographyminutes.com/a-start-to-finish-guide-to-performing-a-breast-ultrasound/},
    abstract = {If you’re new to Breast Ultrasound it can feel overwhelming, but it doesn’t have to be! This step-by-step Breast Ultrasound roadmap will help you conquer this exam, and ensure that you don’t miss anything important along the way. Have you ever faced an Ultrasound exam that you’ve never done before and felt completely lost? As … Continue reading A Start-To-Finish Guide To Performing A Breast Ultrasound},
    language = {en-US},
    urldate = {2025-06-03},
    journal = {Sonography Minutes},
    author = {Macauley, Michelle},
    month = jan,
    year = {2022},
}
@misc{noauthor_cancer_2010,
    type = {{pdqCancerInfoSummary}},
    title = {Cancer {Screening} {Overview} - {NCI}},
    url = {https://www.cancer.gov/about-cancer/screening/patient-screening-overview-pdq},
    abstract = {Cancer screening means looking for cancer before symptoms appear, when cancer may be easier to treat. Screening tests can help reduce the risk of dying from some cancers, but all tests have potential risks, too. Learn more about cancer screening and available tests in this expert-reviewed summary.},
    language = {en},
    urldate = {2025-06-03},
    month = jan,
    year = {2010},
    note = {Archive Location: nciglobal,ncienterprise},
}
@misc{cancer_que_2023,
    title = {Qué es un cribado cáncer de mama - {Blog} {Contra} el {Cáncer}},
    url = {https://blog.contraelcancer.es/cribado-cancer-mama/},
    abstract = {Entra en el blog de la Asociación Española Contra el Cáncer y descubre qué es un cribado de cáncer de mama. ¡Entra ahora y obtén todas tus respuestas!},
    language = {es},
    urldate = {2025-06-03},
    journal = {Blog de la Asociación Española Contra el Cáncer},
    author = {Cáncer, Asociación Española Contra el},
    month = oct,
    year = {2023},
}
@misc{noauthor_ministerio_nodate,
    title = {Ministerio de {Sanidad} - Áreas - {INFORMACION} {GENERAL} - {CRIBADO} {CANCER} - {CANCER} {DE} {MAMA}},
    url = {https://www.sanidad.gob.es/areas/promocionPrevencion/cribado/cribadoCancer/cancerMama/infoGeneral.htm},
    urldate = {2025-06-03},
}
@misc{noauthor_map_nodate,
    title = {Map: {Screening} {Guidelines} by {Country}},
    shorttitle = {Map},
    url = {https://densebreast-info.org/europe/european-screening-guidelines/map-screening-guidelines/},
    language = {en},
    urldate = {2025-06-03},
    journal = {DenseBreast-info, Inc.},
}
@misc{DefinitionBiopsyNCI2011,
    type = {{nciAppModulePage}},
    title = {Definition of biopsy - {NCI} {Dictionary} of {Cancer} {Terms} - {NCI}},
    url = {https://www.cancer.gov/publications/dictionaries/cancer-terms/def/biopsy},
    abstract = {NCI's Dictionary of Cancer Terms provides easy-to-understand definitions for words and phrases related to cancer and medicine.},
    language = {en},
    urldate = {2025-06-03},
    month = feb,
    year = {2011},
    note = {Archive Location: nciglobal,ncienterprise},
}
@misc{noauthor_fine_nodate,
    title = {Fine {Needle} {Aspiration} ({FNA}) of the {Breast}},
    url = {https://www.cancer.org/cancer/types/breast-cancer/screening-tests-and-early-detection/breast-biopsy/fine-needle-aspiration-biopsy-of-the-breast.html},
    abstract = {In an FNA of the breast, a thin needle is used to get a small sample of tissue or fluid to check for cancer cells. Learn more about this type of biopsy here.},
    language = {en},
    urldate = {2025-06-04},
}
@article{jeong_analysis_2020,
    title = {Analysis of the molecular subtypes of preoperative core needle biopsy and surgical specimens in invasive breast cancer},
    volume = {54},
    issn = {2383-7837},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6986971/},
    doi = {10.4132/jptm.2019.10.14},
    abstract = {Background
Accurate molecular classification of breast core needle biopsy (CNB) tissue is important for determining neoadjuvant systemic therapies for invasive breast cancer. The researchers aimed to evaluate the concordance rate (CR) of molecular subtypes between CNBs and surgical specimens.
Methods
This study was conducted with invasive breast cancer patients who underwent surgery after CNB at Seoul St. Mary’s Hospital between December 2014 and December 2017. Estrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2), and Ki67 were analyzed using immunohistochemistry. ER and PR were evaluated by Allred score (0–8). HER2 was graded from 0 to +3, and all 2+ cases were reflex tested with silver in situ hybridization. The labeling index of Ki67 was counted by either manual scoring or digital image analysis. Molecular subtypes were classified using the above surrogate markers.
Results
In total, 629 patients were evaluated. The CRs of ER, PR, HER2, and Ki67 were 96.5\% (kappa, 0.883; p{\textless}.001), 93.0\% (kappa, 0.824; p{\textless}.001), 99.7\% (kappa, 0.988; p{\textless}.001), and 78.7\% (kappa, 0.577; p{\textless}.001), respectively. Digital image analysis of Ki67 in CNB showed better concordance with Ki67 in surgical specimens (CR, 82.3\%; kappa, 0.639 for digital image analysis vs. CR, 76.2\%; kappa, 0.534 for manual counting). The CRs of luminal A, luminal B, HER2, and triple negative types were 89.0\%, 70.0\%, 82.9\%, and 77.2\%, respectively. 
Conclusions
CNB was reasonably accurate for determining ER, PR, HER2, Ki67, and molecular subtypes. Using digital image analysis for Ki67 in CNB produced more accurate molecular classifications.},
    number = {1},
    urldate = {2025-06-04},
    journal = {Journal of Pathology and Translational Medicine},
    author = {Jeong, Ye Sul and Kang, Jun and Lee, Jieun and Yoo, Tae-Kyung and Kim, Sung Hun and Lee, Ahwon},
    month = jan,
    year = {2020},
    pmid = {31718121},
    pmcid = {PMC6986971},
    pages = {87--94},
}
@misc{CoreNeedleBiopsy,
    title = {Core {Needle} {Biopsy} of the {Breast} {\textbar} {Stereotactic} {Breast} {Biopsy}},
    url = {https://www.cancer.org/cancer/types/breast-cancer/screening-tests-and-early-detection/breast-biopsy/core-needle-biopsy-of-the-breast.html},
    abstract = {Core needle biopsy (CNB) uses a hollow needle to remove pieces of breast tissue to check for cancer cells. Learn about the types of CNB \& what to expect.},
    language = {en},
    urldate = {2025-06-04},
}
@article{park_vacuum-assisted_2014,
    title = {Vacuum-assisted breast biopsy for breast cancer},
    volume = {3},
    issn = {2227-8575, 2227-684X},
    url = {https://gs.amegroups.org/article/view/3687},
    doi = {10.3978/j.issn.2227-684X.2014.02.03},
    abstract = {Vacuum-assisted breast biopsy for breast cancer},
    language = {en},
    number = {2},
    urldate = {2025-06-04},
    journal = {Gland Surgery},
    author = {Park, Hai-Lin and Hong, Jisun},
    month = may,
    year = {2014},
    note = {Number: 2
Publisher: AME Publishing Company},
    pages = {12027--12127},
}
@article{silva_breast_2023,
    title = {Breast biopsy techniques in a global setting—clinical practice review},
    volume = {4},
    issn = {2218-6778},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11093018/},
    doi = {10.21037/tbcr-23-12},
    abstract = {Breast cancer is a disease of global concern, regardless of economic status. A significant disparity in breast cancer care between low- and high-income countries is not unexpected, but consideration can be given to particular aspects of therapy to allow as much equitability as possible. One of these aspects involves biopsy of breast lesions. With available resources, management in developed countries focuses on dealing with screening and image-detected lesions. In such circumstances, advanced percutaneous biopsy techniques are utilized liberally. However, where resources are less forthcoming for mammographic screening, women frequently present with symptomatic, palpable and larger tumours. This scenario behooves the clinician to modify treatment approaches and yet use cost-effective management strategies. It is essential that thought is applied to breast biopsy technique used where there is cost-consciousness as it significantly influences subsequent therapy. Less expensive strategies like fine needle aspiration cytology (FNAC) and core needle biopsy (CNB), when performed with particular attention to technique, handling, transportation and preparation of biopsy specimens allows a high level of accuracy and provides adequate information for the next steps in treatment. This mini-review discusses the variation in biopsy approaches among lower and higher income areas and offers suggestions for appropriate breast biopsy strategies in resource-limited countries.},
    urldate = {2025-06-04},
    journal = {Translational Breast Cancer Research},
    author = {Silva, Edibaldo and Meschter, Steven and Tan, Mona P.},
    month = apr,
    year = {2023},
    pmid = {38751462},
    pmcid = {PMC11093018},
    pages = {14},
}
@misc{colestrykerWhatArtificialIntelligence2024,
    title = {What {Is} {Artificial} {Intelligence} ({AI})? {\textbar} {IBM}},
    shorttitle = {What {Is} {Artificial} {Intelligence} ({AI})?},
    url = {https://www.ibm.com/think/topics/artificial-intelligence},
    abstract = {Artificial intelligence (AI) is technology that enables computers and machines to simulate human learning, comprehension, problem solving, decision-making, creativity and autonomy.},
    language = {en},
    urldate = {2025-06-04},
    author = {{Cole Stryker} and {Eda Kavlakoglu}},
    month = aug,
    year = {2024},
}
@misc{filipsson_evolution_2024,
    title = {The {Evolution} of {AI}: {Tracing} its {Roots} and {Milestones}},
    shorttitle = {The {Evolution} of {AI}},
    url = {https://redresscompliance.com/the-evolution-of-ai-tracing-its-roots-and-milestones/},
    abstract = {Discover the Evolution of AI: From rule-based systems to advanced NLP and computer vision. Explore the societal impacts and ethical challenges of AI.},
    language = {en-US},
    urldate = {2025-06-04},
    author = {Filipsson, Fredrik},
    month = jan,
    year = {2024},
}
@misc{noauthor_what_nodate,
    title = {What is machine learning? {Understanding} types \& applications - {Spiceworks}},
    shorttitle = {What is machine learning?},
    url = {https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-ml/},
    abstract = {Machine learning is the science of computer algorithms that help machines learn and improve from data analysis without explicit programming.},
    language = {en-US},
    urldate = {2025-06-04},
    journal = {Spiceworks Inc},
}
@article{malik_credit_2024,
    title = {Credit {Risk} {Assessment} and {Fraud} {Detection} in {Financial} {Transactions} {Using} {Machine} {Learning}},
    volume = {20},
    copyright = {https://creativecommons.org/licenses/by-nd/4.0},
    issn = {1112-5209},
    url = {https://journal.esrgroups.org/jes/article/view/1807},
    doi = {10.52783/jes.1807},
    abstract = {Credit risk assessment and fraud detection are crucial tasks in the financial industry, vital to preserving financial organizations' legitimacy and sustainability. Traditional methods often fall short in accurately assessing risk and detecting fraudulent activities in a timely manner. In recent years, machine learning has emerged as a powerful tool for enhancing these processes, leveraging great dimensions of transactional statistics and superior algos for making more informed decisions. This research paper explores the usage of ML techniques in credit risk assessment and fraud detection within financial transactions.
The paper begins with an overview of the importance of accurate risk assessment and fraud detection in financial transactions and introduces the role of machine learning in addressing these challenges. A comprehensive literature review is conducted to analyze existing methodologies, algorithms, and research trends in the field. Data acquisition and preprocessing techniques are discussed, emphasizing the importance of clean and relevant data for model training. Feature engineering strategies are explored to extract meaningful information from financial transaction data and enhance the predictive capabilities of machine learning models.
Various machine learning algorithms suitable for credit risk assessment and fraud detection are examined, including LR, SVMs, RF, DTs and DNNs. The efficacy of these techniques is evaluated by discussing model metrics for assessment and ensemble approaches for boosting efficiency, with a focus on metrics such as accuracy, precision, recall, and ROC-AUC.
The paper presents case studies and experimental results illustrating the application of machine learning models in real-world scenarios, highlighting their effectiveness in improving risk assessment and fraud detection processes. Additionally, difficulties such as imbalanced datasets, comprehensibility of the model and adherence to regulations are discussed, along with potential research directions and future trends in the field.
In conclusion, this research emphasizes the transformative potential of machine learning in credit risk assessment and fraud detection within financial transactions. By leveraging advanced algorithms and data-driven approaches, financial institutions can enhance their decision-making processes, mitigate risks, and safeguard against fraudulent activities, ultimately contributing to a more secure and resilient financial ecosystem.},
    language = {en},
    number = {3s},
    urldate = {2025-06-04},
    journal = {Journal of Electrical Systems},
    author = {Malik, Pankaj and Chourasia, Ankita and Pandit, Rakesh and Bawane, Sheetal and Surana, Jayesh},
    month = mar,
    year = {2024},
    note = {Number: 3s},
    keywords = {ROC-AUC., financial ecosystem, fraudulent, secure},
    pages = {2061--2069},
}
@misc{noauthor_machine_nodate,
    title = {Machine learning in healthcare: {Uses}, benefits and pioneers in the field},
    shorttitle = {Machine learning in healthcare},
    url = {https://eithealth.eu/news-article/machine-learning-in-healthcare-uses-benefits-and-pioneers-in-the-field/},
    abstract = {Explore the transformative impact of machine learning in healthcare, from disease diagnosis to personalised treatment plans.},
    language = {en-US},
    urldate = {2025-06-04},
    journal = {EIT Health},
}
@misc{holdsworthWhatDeepLearning2024,
    title = {What {Is} {Deep} {Learning}? {\textbar} {IBM}},
    shorttitle = {What {Is} {Deep} {Learning}?},
    url = {https://www.ibm.com/think/topics/deep-learning},
    abstract = {Deep learning is a subset of machine learning that uses multilayered neural networks, to simulate the complex decision-making power of the human brain.},
    language = {en},
    urldate = {2025-06-04},
    author = {Holdsworth, Jim and Scapicchio, Mark},
    month = jun,
    year = {2024},
}
@article{turing_icomputing_1950,
    title = {I.—{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
    volume = {LIX},
    issn = {0026-4423},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    doi = {10.1093/mind/LIX.236.433},
    number = {236},
    urldate = {2025-06-04},
    journal = {Mind},
    author = {TURING, A. M.},
    month = oct,
    year = {1950},
    pages = {433--460},
}
@article{miotto_deep_2017,
    title = {Deep learning for healthcare: review, opportunities and challenges},
    volume = {19},
    issn = {1467-5463},
    shorttitle = {Deep learning for healthcare},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6455466/},
    doi = {10.1093/bib/bbx044},
    abstract = {Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.},
    number = {6},
    urldate = {2025-06-04},
    journal = {Briefings in Bioinformatics},
    author = {Miotto, Riccardo and Wang, Fei and Wang, Shuang and Jiang, Xiaoqian and Dudley, Joel T},
    month = may,
    year = {2017},
    pmid = {28481991},
    pmcid = {PMC6455466},
    pages = {1236--1246},
}
@misc{nasaWhatArtificialIntelligence,
    title = {What is {Artificial} {Intelligence}? - {NASA}},
    shorttitle = {What is {Artificial} {Intelligence}?},
    url = {https://www.nasa.gov/what-is-artificial-intelligence/},
    abstract = {Artificial intelligence refers to computer systems that can perform complex tasks normally done by human-reasoning, decision making, creating, etc.},
    language = {en-US},
    urldate = {2025-06-04},
    author = {{NASA}},
}
@article{jiang_supervised_2020,
    title = {Supervised machine learning: {A} brief primer},
    volume = {51},
    issn = {0005-7894},
    shorttitle = {Supervised machine learning},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7431677/},
    doi = {10.1016/j.beth.2020.05.002},
    abstract = {Machine learning is increasingly used in mental health research and has the potential to advance our understanding of how to characterize, predict, and treat mental disorders and associated adverse health outcomes (e.g., suicidal behavior). Machine learning offers new tools to overcome challenges for which traditional statistical methods are not well-suited. This manuscript provides an overview of machine learning with a specific focus on supervised learning (i.e., methods that are designed to predict or classify an outcome of interest). Several common supervised learning methods are described, along with applied examples from the published literature. We also provide an overview of supervised learning model building, validation, and performance evaluation. Finally, challenges in creating robust and generalizable machine learning algorithms are discussed.},
    number = {5},
    urldate = {2025-06-04},
    journal = {Behavior therapy},
    author = {Jiang, Tammy and Gradus, Jaimie L. and Rosellini, Anthony J.},
    month = sep,
    year = {2020},
    pmid = {32800297},
    pmcid = {PMC7431677},
    pages = {675--687},
}
@misc{noauthor_unsupervised_nodate,
    title = {Unsupervised {Learning} - an overview {\textbar} {ScienceDirect} {Topics}},
    url = {https://www.sciencedirect.com/topics/computer-science/unsupervised-learning},
    urldate = {2025-06-04},
}
@misc{ghasemi_introduction_2024,
    title = {Introduction to {Reinforcement} {Learning}},
    url = {http://arxiv.org/abs/2408.07712},
    doi = {10.48550/arXiv.2408.07712},
    abstract = {Reinforcement Learning (RL), a subfield of Artificial Intelligence (AI), focuses on training agents to make decisions by interacting with their environment to maximize cumulative rewards. This paper provides an overview of RL, covering its core concepts, methodologies, and resources for further learning. It offers a thorough explanation of fundamental components such as states, actions, policies, and reward signals, ensuring readers develop a solid foundational understanding. Additionally, the paper presents a variety of RL algorithms, categorized based on the key factors such as model-free, model-based, value-based, policy-based, and other key factors. Resources for learning and implementing RL, such as books, courses, and online communities are also provided. By offering a clear, structured introduction, this paper aims to simplify the complexities of RL for beginners, providing a straightforward pathway to understanding.},
    urldate = {2025-06-04},
    publisher = {arXiv},
    author = {Ghasemi, Majid and Ebrahimi, Dariush},
    month = dec,
    year = {2024},
    note = {arXiv:2408.07712 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}
@article{lecun_gradient-based_1998,
    title = {Gradient-based learning applied to document recognition},
    volume = {86},
    issn = {1558-2256},
    url = {https://ieeexplore.ieee.org/document/726791/authors},
    doi = {10.1109/5.726791},
    abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
    number = {11},
    urldate = {2025-06-04},
    journal = {Proceedings of the IEEE},
    author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
    month = nov,
    year = {1998},
    keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
    pages = {2278--2324},
}
@inproceedings{NIPS2012_c399862d,
    title = {{ImageNet} classification with deep convolutional neural networks},
    volume = {25},
    url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
    booktitle = {Advances in neural information processing systems},
    publisher = {Curran Associates, Inc.},
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
    editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
    year = {2012},
}
@misc{noauthor_what_2021,
    title = {What are {Convolutional} {Neural} {Networks}? {\textbar} {IBM}},
    shorttitle = {What are {Convolutional} {Neural} {Networks}?},
    url = {https://www.ibm.com/think/topics/convolutional-neural-networks},
    abstract = {Convolutional neural networks use three-dimensional data to for image classification and object recognition tasks.},
    language = {en},
    urldate = {2025-06-04},
    month = oct,
    year = {2021},
}
@misc{langActivationFunctionsNeural2024,
    title = {Activation {Functions} in {Neural} {Networks}: {How} to {Choose} the {Right} {One}},
    shorttitle = {Activation {Functions} in {Neural} {Networks}},
    url = {https://towardsdatascience.com/activation-functions-in-neural-networks-how-to-choose-the-right-one-cb20414c04e5/},
    abstract = {Introduction to activation functions and an overview of the most famous functions},
    language = {en-US},
    urldate = {2025-06-04},
    journal = {Towards Data Science},
    author = {Lang, Niklas},
    month = dec,
    year = {2024},
}
@misc{wachtelUnderstandingActivationFunctions2021,
    title = {Understanding {Activation} {Functions} {\textbar} {Data} {Science} for the {Rest} of {Us}},
    url = {https://medium.com/analytics-vidhya/understanding-activation-functions-data-science-for-the-rest-of-us-b652048a064f},
    abstract = {A bite-size introduction to activation functions for those new to neural nets and/or who hate math…},
    language = {en},
    urldate = {2025-06-04},
    journal = {Analytics Vidhya},
    author = {Wachtel, Ben},
    month = jun,
    year = {2021},
}
@misc{brownlee_gentle_2019,
    title = {A {Gentle} {Introduction} to {Pooling} {Layers} for {Convolutional} {Neural} {Networks}},
    url = {https://www.machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/},
    abstract = {Convolutional layers in a convolutional neural network summarize the presence of features in an input image. A problem with the output feature maps is that they are sensitive to the location of the features in the input. One approach to address this sensitivity is to down sample the feature maps. This has the effect of […]},
    language = {en-US},
    urldate = {2025-06-04},
    journal = {MachineLearningMastery.com},
    author = {Brownlee, Jason},
    month = apr,
    year = {2019},
}
@misc{SkinLesionClassification,
    title = {Skin {Lesion} {Classification} {Using} {Deep} {Neural} {Network}},
    url = {https://www.researchgate.net/publication/337336341_Skin_Lesion_Classification_Using_Deep_Neural_Network},
    abstract = {PDF {\textbar} This paper reports the methods and techniques we have developed for classify dermoscopic images (task 1) of the ISIC 2019 challenge dataset for... {\textbar} Find, read and cite all the research you need on ResearchGate},
    language = {en},
    urldate = {2025-06-04},
    journal = {ResearchGate},
    doi = {10.48550/arXiv.1911.07817},
}
@misc{noauthor_fully_nodate,
    title = {Fully {Connected} {Layer} vs. {Convolutional} {Layer}: {Explained}},
    shorttitle = {Fully {Connected} {Layer} vs. {Convolutional} {Layer}},
    url = {https://builtin.com/machine-learning/fully-connected-layer},
    abstract = {A fully connected layer is a neural network layer where each input node is connected to each output node. In a convolutional layer, not all nodes are connected. Here’s what you need to know.},
    language = {en},
    urldate = {2025-06-04},
    journal = {Built In},
}
@misc{noauthor_convolutional_nodate,
    title = {Convolutional {Neural} {Networks} ({CNN}) {Overview}},
    url = {https://encord.com/blog/convolutional-neural-networks-explained/},
    abstract = {Convolutional Neural Networks (CNNs) are a powerful tool for image analysis that can be used for tasks such as image classification, object dete},
    language = {en-GB},
    urldate = {2025-06-04},
}
@misc{bergmann_what_2024,
    title = {What is {Backpropagation}? {\textbar} {IBM}},
    shorttitle = {What is {Backpropagation}?},
    url = {https://www.ibm.com/think/topics/backpropagation},
    abstract = {Backpropagation is a machine learning algorithm for training neural networks by using the chain rule to compute how network weights contribute to a loss function.},
    language = {en},
    urldate = {2025-06-05},
    author = {Bergmann, Dave and Stryker, Cole},
    month = nov,
    year = {2024},
}
@article{jaamour_divide_2023,
    title = {A divide and conquer approach to maximise deep learning mammography classification accuracies},
    volume = {18},
    issn = {1932-6203},
    url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280841},
    doi = {10.1371/journal.pone.0280841},
    abstract = {Breast cancer claims 11,400 lives on average every year in the UK, making it one of the deadliest diseases. Mammography is the gold standard for detecting early signs of breast cancer, which can help cure the disease during its early stages. However, incorrect mammography diagnoses are common and may harm patients through unnecessary treatments and operations (or a lack of treatment). Therefore, systems that can learn to detect breast cancer on their own could help reduce the number of incorrect interpretations and missed cases. Various deep learning techniques, which can be used to implement a system that learns how to detect instances of breast cancer in mammograms, are explored throughout this paper. Convolution Neural Networks (CNNs) are used as part of a pipeline based on deep learning techniques. A divide and conquer approach is followed to analyse the effects on performance and efficiency when utilising diverse deep learning techniques such as varying network architectures (VGG19, ResNet50, InceptionV3, DenseNet121, MobileNetV2), class weights, input sizes, image ratios, pre-processing techniques, transfer learning, dropout rates, and types of mammogram projections. This approach serves as a starting point for model development of mammography classification tasks. Practitioners can benefit from this work by using the divide and conquer results to select the most suitable deep learning techniques for their case out-of-the-box, thus reducing the need for extensive exploratory experimentation. Multiple techniques are found to provide accuracy gains relative to a general baseline (VGG19 model using uncropped 512 × 512 pixels input images with a dropout rate of 0.2 and a learning rate of 1 × 10−3) on the Curated Breast Imaging Subset of DDSM (CBIS-DDSM) dataset. These techniques involve transfer learning pre-trained ImagetNet weights to a MobileNetV2 architecture, with pre-trained weights from a binarised version of the mini Mammography Image Analysis Society (mini-MIAS) dataset applied to the fully connected layers of the model, coupled with using weights to alleviate class imbalance, and splitting CBIS-DDSM samples between images of masses and calcifications. Using these techniques, a 5.6\% gain in accuracy over the baseline model was accomplished. Other deep learning techniques from the divide and conquer approach, such as larger image sizes, do not yield increased accuracies without the use of image pre-processing techniques such as Gaussian filtering, histogram equalisation and input cropping.},
    language = {en},
    number = {5},
    urldate = {2025-06-05},
    journal = {PLOS ONE},
    author = {Jaamour, Adam and Myles, Craig and Patel, Ashay and Chen, Shuen-Jen and McMillan, Lewis and Harris-Birtill, David},
    year = {2023},
    note = {Publisher: Public Library of Science},
    keywords = {Breast cancer, Calcification, Cancer detection and diagnosis, Deep learning, Imaging techniques, Machine learning, Malignant tumors, Mammography},
    pages = {e0280841},
}
@misc{he_deep_2015,
    title = {Deep {Residual} {Learning} for {Image} {Recognition}},
    url = {http://arxiv.org/abs/1512.03385},
    doi = {10.48550/arXiv.1512.03385},
    abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
    urldate = {2025-06-05},
    publisher = {arXiv},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    month = dec,
    year = {2015},
    note = {arXiv:1512.03385 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{simonyanVeryDeepConvolutional,
    title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
    url = {https://arxiv.org/abs/1409.1556},
    urldate = {2025-06-05},
    author = {Simonyan, Karen and Zisserman, Andrew},
}
@misc{pereira_review_2024,
    title = {A {Review} of {Transformer}-{Based} {Models} for {Computer} {Vision} {Tasks}: {Capturing} {Global} {Context} and {Spatial} {Relationships}},
    shorttitle = {A {Review} of {Transformer}-{Based} {Models} for {Computer} {Vision} {Tasks}},
    url = {http://arxiv.org/abs/2408.15178},
    doi = {10.48550/arXiv.2408.15178},
    abstract = {Transformer-based models have transformed the landscape of natural language processing (NLP) and are increasingly applied to computer vision tasks with remarkable success. These models, renowned for their ability to capture long-range dependencies and contextual information, offer a promising alternative to traditional convolutional neural networks (CNNs) in computer vision. In this review paper, we provide an extensive overview of various transformer architectures adapted for computer vision tasks. We delve into how these models capture global context and spatial relationships in images, empowering them to excel in tasks such as image classification, object detection, and segmentation. Analyzing the key components, training methodologies, and performance metrics of transformer-based models, we highlight their strengths, limitations, and recent advancements. Additionally, we discuss potential research directions and applications of transformer-based models in computer vision, offering insights into their implications for future advancements in the field.},
    urldate = {2025-06-05},
    publisher = {arXiv},
    author = {Pereira, Gracile Astlin and Hussain, Muhammad},
    month = aug,
    year = {2024},
    note = {arXiv:2408.15178 [cs]
version: 1},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{vaswani_attention_2017,
    title = {Attention {Is} {All} {You} {Need}},
    url = {http://arxiv.org/abs/1706.03762},
    doi = {10.48550/arXiv.1706.03762},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
    urldate = {2025-06-05},
    publisher = {arXiv},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    month = jun,
    year = {2017},
    note = {arXiv:1706.03762 [cs]
version: 1},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}
@misc{murel_what_2024,
    title = {What is transfer learning? {\textbar} {IBM}},
    shorttitle = {What is transfer learning?},
    url = {https://www.ibm.com/think/topics/transfer-learning},
    abstract = {What is transfer learning? Learn how this machine learning technique fixes improves model generalizability and performance.},
    language = {en},
    urldate = {2025-06-05},
    author = {Murel, Jacob and Kavlakoglu, Eva},
    month = feb,
    year = {2024},
}
@misc{matsoukas_what_2022,
    title = {What {Makes} {Transfer} {Learning} {Work} {For} {Medical} {Images}: {Feature} {Reuse} \& {Other} {Factors}},
    shorttitle = {What {Makes} {Transfer} {Learning} {Work} {For} {Medical} {Images}},
    url = {http://arxiv.org/abs/2203.01825},
    doi = {10.48550/arXiv.2203.01825},
    abstract = {Transfer learning is a standard technique to transfer knowledge from one domain to another. For applications in medical imaging, transfer from ImageNet has become the de-facto approach, despite differences in the tasks and image characteristics between the domains. However, it is unclear what factors determine whether - and to what extent - transfer learning to the medical domain is useful. The long-standing assumption that features from the source domain get reused has recently been called into question. Through a series of experiments on several medical image benchmark datasets, we explore the relationship between transfer learning, data size, the capacity and inductive bias of the model, as well as the distance between the source and target domain. Our findings suggest that transfer learning is beneficial in most cases, and we characterize the important role feature reuse plays in its success.},
    urldate = {2025-06-06},
    publisher = {arXiv},
    author = {Matsoukas, Christos and Haslum, Johan Fredin and Sorkhei, Moein and Söderberg, Magnus and Smith, Kevin},
    month = jun,
    year = {2022},
    note = {arXiv:2203.01825 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}
@misc{noauthor_what_2024,
    title = {What is {Fine}-{Tuning}? {\textbar} {IBM}},
    shorttitle = {What is {Fine}-{Tuning}?},
    url = {https://www.ibm.com/think/topics/fine-tuning},
    abstract = {Fine-tuning in machine learning is the process of adapting a pre-trained model for specific tasks or use cases through further training on a smaller dataset.},
    language = {en},
    urldate = {2025-06-06},
    month = mar,
    year = {2024},
}
@misc{noauthor_31_nodate,
    title = {3.1. {Cross}-validation: evaluating estimator performance},
    shorttitle = {3.1. {Cross}-validation},
    url = {https://scikit-learn/stable/modules/cross_validation.html},
    abstract = {Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would ha...},
    language = {en},
    urldate = {2025-06-06},
    journal = {scikit-learn},
}
@misc{amino_pros_2024,
    title = {Pros and {Cons} of {Liquid} {Versus} {Tissue} {Biopsy} in {Cancer} {Research}},
    url = {https://iprocess.net/liquid-biopsy-vs-tissue-biopsy/},
    abstract = {Discover the advantages and drawbacks of liquid and tissue biopsies in cancer research. Learn which method suits your investigative needs best.},
    language = {en-US},
    urldate = {2025-06-06},
    journal = {iProcess},
    author = {amino},
    month = feb,
    year = {2024},
}
@misc{howardMobileNetsEfficientConvolutional2017,
    title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
    shorttitle = {{MobileNets}},
    url = {http://arxiv.org/abs/1704.04861},
    doi = {10.48550/arXiv.1704.04861},
    abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
    urldate = {2025-06-05},
    publisher = {arXiv},
    author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
    month = apr,
    year = {2017},
    note = {arXiv:1704.04861 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{bahdanau_neural_2016,
    title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
    url = {http://arxiv.org/abs/1409.0473},
    doi = {10.48550/arXiv.1409.0473},
    abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
    urldate = {2025-06-06},
    publisher = {arXiv},
    author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
    month = may,
    year = {2016},
    note = {arXiv:1409.0473 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}
@misc{bahdanau_neural_2014,
    title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
    url = {http://arxiv.org/abs/1409.0473},
    doi = {10.48550/arXiv.1409.0473},
    abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
    urldate = {2025-06-06},
    publisher = {arXiv},
    author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
    month = sep,
    year = {2014},
    note = {arXiv:1409.0473 [cs]
version: 1},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}
@misc{dosovitskiy_image_2020,
    title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
    shorttitle = {An {Image} is {Worth} 16x16 {Words}},
    url = {http://arxiv.org/abs/2010.11929},
    doi = {10.48550/arXiv.2010.11929},
    abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
    urldate = {2025-06-06},
    publisher = {arXiv},
    author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
    month = oct,
    year = {2020},
    note = {arXiv:2010.11929 [cs]
version: 1},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
@article{abimouloud_vision_2024,
    title = {Vision transformer-convolution for breast cancer classification using mammography images: {A} comparative study},
    volume = {20},
    issn = {1448-5869},
    shorttitle = {Vision transformer-convolution for breast cancer classification using mammography images},
    url = {https://journals.sagepub.com/action/showAbstract},
    doi = {10.3233/HIS-240002},
    abstract = {Breast cancer is a significant global health concern, highlighting the critical importance of early detection for effective treatment of women’s health. While convolutional networks (CNNs) have been the best for analysing medical images, recent interest has emerged in leveraging vision transformers (ViTs) for medical data analysis. This study aimed to conduct a comprehensive comparison of three systems a self-attention transformer (VIT), a compact convolution transformer (CCT), and a tokenlearner (TVIT) for binary classification of mammography images into benign and cancerous tissue. Thorough experiments were performed using the DDSM dataset, which consists of 5970 benign and 7158 malignant images. The performance accuracy of the proposed models was evaluated, yielding results of 99.81\% for VIT, 99.92\% for CCT, and 99.05\% for TVIT. Additionally, the study compared these results with the current state-of-the-art performance metrics. The findings demonstrate how convolution-attention mechanisms can effectively contribute to the development of robust computer-aided systems for diagnosing breast cancer. Notably, the proposed approach achieves high-performance results while also minimizing the computational resources required and reducing decision time.},
    language = {EN},
    number = {2},
    urldate = {2025-06-06},
    journal = {International Journal of Hybrid Intelligent Systems},
    author = {Abimouloud, Mouhamed Laid and Bensid, Khaled and Elleuch, Mohamed and Aiadi, Oussama and Kherallah, Monji},
    month = may,
    year = {2024},
    note = {Publisher: SAGE Publications},
    pages = {67--83},
}

@article{ayana_vision-transformer-based_2023,
    title = {Vision-{Transformer}-{Based} {Transfer} {Learning} for {Mammogram} {Classification}},
    volume = {13},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {2075-4418},
    url = {https://www.mdpi.com/2075-4418/13/2/178},
    doi = {10.3390/diagnostics13020178},
    abstract = {Breast mass identification is a crucial procedure during mammogram-based early breast cancer diagnosis. However, it is difficult to determine whether a breast lump is benign or cancerous at early stages. Convolutional neural networks (CNNs) have been used to solve this problem and have provided useful advancements. However, CNNs focus only on a certain portion of the mammogram while ignoring the remaining and present computational complexity because of multiple convolutions. Recently, vision transformers have been developed as a technique to overcome such limitations of CNNs, ensuring better or comparable performance in natural image classification. However, the utility of this technique has not been thoroughly investigated in the medical image domain. In this study, we developed a transfer learning technique based on vision transformers to classify breast mass mammograms. The area under the receiver operating curve of the new model was estimated as 1 ± 0, thus outperforming the CNN-based transfer-learning models and vision transformer models trained from scratch. The technique can, hence, be applied in a clinical setting, to improve the early diagnosis of breast cancer.},
    language = {en},
    number = {2},
    urldate = {2025-06-06},
    journal = {Diagnostics},
    author = {Ayana, Gelan and Dese, Kokeb and Dereje, Yisak and Kebede, Yonas and Barki, Hika and Amdissa, Dechassa and Husen, Nahimiya and Mulugeta, Fikadu and Habtamu, Bontu and Choe, Se-Woon},
    month = jan,
    year = {2023},
    note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
    keywords = {breast cancer, mammography, transfer learning, transformers},
    pages = {178},
}
@misc{liu_swin_2021,
    title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
    shorttitle = {Swin {Transformer}},
    url = {http://arxiv.org/abs/2103.14030},
    doi = {10.48550/arXiv.2103.14030},
    abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The code and models will be made publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
    urldate = {2025-06-06},
    publisher = {arXiv},
    author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
    month = mar,
    year = {2021},
    note = {arXiv:2103.14030 [cs]
version: 1},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
@article{marquez_vara_vision_2024,
    title = {Vision transformer for classifying benign and malignant breast tumors in mammography},
    copyright = {memòria: cc-nc-nd (c) Noah Márquez Vara, 2024},
    url = {https://diposit.ub.edu/dspace/handle/2445/216430},
    abstract = {[en] The need for precise diagnostic tools to distinguish between benign and malignant breast cancers is underscored by the fact that breast cancer remains a major global health concern. This is the reason why the use of Vision Transformers (ViTs) to categorize breast cancers in mammography is studied. Using the OPTIMAM Medical Database, which includes mammography scans from the UK National Health Service Breast Screening Program, we assess how well ViTs perform on this particular assignment.
The methodology includes proper preprocessing, augmentation, and splitting of data, including significant model training for fine-tuning hyperparameters concerning the prevention of data leakage. Insightful metrics such as AUC-ROC are used in evaluating correctly the model’s performance. The results make ViTs a state-of-the-art alternative to CNNs due to their capacity to capture global context through self-attention mechanisms. This is especially useful in complex tasks like medical imaging interpretation. The potential of cutting-edge AI methods to improve diagnostic precision and enhance patient outcomes during breast cancer diagnosis is highlighted by this study.
[es] La necessitat d’eines de diagnòstic precises per distingir entre càncers de mama benignes i malignes destaca pel fet que el càncer de mama continua sent una preocupació important globalment. És per això que s’estudia l’ús de Vision Transformers (ViTs) per categoritzar els càncers de mama en mamografies. Utilitzant la base de dades mèdica d’OPTIMAM, que inclou mamografies del Programa de Detecció de Càncer de Mama del Servei Nacional de Salut del Regne Unit, avaluem que tan bé funcionen els ViTs en aquesta tasca específica.
La metodologia inclou un adequat preprocessament, augment i divisió de les dades, incloent-hi un entrenament significatiu del model per ajustar els hiperparàmetres en relació amb la prevenció de la contaminació de dades. Mètriques significatives i intuïtives com l’AUC-ROC s’utilitzen per avaluar correctament el rendiment del model. Els resultats converteixen els ViTs en una alternativa de l’estat de l’art a les CNN a causa de la seva capacitat per capturar el context global a través de mecanismes d’autoatenció. Això és especialment útil en tasques complexes, com ara la interpretació d’imatges mèdiques. El potencial dels mètodes avançats d’IA per millorar la precisió diagnòstica i millorar els resultats dels pacients durant el diagnòstic de càncer de mama es destaca en aquest estudi.},
    language = {eng},
    urldate = {2025-06-06},
    journal = {Treballs Finals de Grau (TFG) - Enginyeria Informàtica},
    author = {Márquez Vara, Noah},
    month = jun,
    year = {2024},
    note = {Accepted: 2024-11-13T10:03:57Z},
}
@misc{tu_maxvit_2022,
    title = {{MaxViT}: {Multi}-{Axis} {Vision} {Transformer}},
    shorttitle = {{MaxViT}},
    url = {http://arxiv.org/abs/2204.01697},
    doi = {10.48550/arXiv.2204.01697},
    abstract = {Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to "see" globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5{\textbackslash}\% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7{\textbackslash}\% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. We will make the code and models publicly available.},
    urldate = {2025-06-06},
    publisher = {arXiv},
    author = {Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
    month = apr,
    year = {2022},
    note = {arXiv:2204.01697 [cs]
version: 1},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
@misc{noauthor_maxvit-unet_2024,
    title = {{MAXVIT}-{UNET}: {MULTI}-{AXIS} {ATTENTION} {FOR} {MEDICAL} {IMAGE} {SEGMENTATION}},
    shorttitle = {{MAXVIT}-{UNET}},
    url = {https://blogs.torus.ai/maxvit-unet-multi-axis-attention-for-medical-image/},
    abstract = {Since their emergence, Convolutional Neural Networks (CNNs) have significantly advanced medical image analysis but struggle with capturing global interactions. Transformers excel in processing global features but face scalability and bias issues. To address these issues, hybrid vision transformers (CNN-Transformers) have been developed, combining the strengths of both convolution and self-attention},
    language = {en},
    urldate = {2025-06-06},
    journal = {Torus AI blogs},
    month = jul,
    year = {2024},
}

@misc{PythonDataAnalysis,
    title = {Python {Data} {Analysis} {Library}},
    url = {https://pandas.pydata.org/},
    urldate = {2025-06-07},
}
@misc{PyTorch,
    title = {{PyTorch}},
    url = {https://pytorch.org/},
    abstract = {PyTorch Foundation is the deep learning community home for the open source PyTorch framework and ecosystem.},
    language = {en-US},
    urldate = {2025-06-07},
    journal = {PyTorch},
}
@misc{ScikitlearnMachineLearning,
    title = {scikit-learn: machine learning in {Python}},
    url = {https://scikit-learn.org/stable/index.html},
    urldate = {2025-06-07},
}
@misc{WeightsBiasesAI,
    title = {Weights \& {Biases}: {The} {AI} {Developer} {Platform}},
    url = {https://wandb.ai/site},
    urldate = {2025-06-07},
}
@misc{NumPy,
    title = {{NumPy}},
    url = {https://numpy.org/doc/stable/index.html},
    urldate = {2025-06-07},
}
@misc{Pydicom,
    title = {Pydicom},
    url = {https://pydicom.github.io/},
    urldate = {2025-06-07},
}
@misc{OpenCV,
    title = {{OpenCV}},
    url = {https://opencv.org/},
    abstract = {OpenCV provides a real-time optimized Computer Vision library, tools, and hardware. It also supports model execution for Machine Learning (ML) and Artificial Intelligence (AI).},
    language = {en-US},
    urldate = {2025-06-07},
    journal = {OpenCV},
}
@misc{PyTorchLightning,
    title = {{PyTorch} {Lightning}},
    url = {https://lightning.ai/docs/pytorch/stable/},
    urldate = {2025-06-07},
}
@misc{TorchMetrics,
    title = {{TorchMetrics}},
    url = {https://lightning.ai/docs/torchmetrics/stable/},
    urldate = {2025-06-07},
}
@misc{AlbumentationsFastFlexible,
    title = {Albumentations: fast and flexible image augmentations},
    shorttitle = {Albumentations},
    url = {https://albumentations.ai/},
    abstract = {Improve computer vision models with Albumentations, the fast and flexible Python library for high-performance image augmentation. Supports images, masks, bounding boxes, keypoints \& easy framework integration.},
    language = {en},
    urldate = {2025-06-07},
    journal = {Albumentations},
}
@article{waskomSeabornStatisticalData2021,
    title = {seaborn: statistical data visualization},
    volume = {6},
    issn = {2475-9066},
    shorttitle = {seaborn},
    url = {https://joss.theoj.org/papers/10.21105/joss.03021},
    doi = {10.21105/joss.03021},
    abstract = {Waskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021, https://doi.org/10.21105/joss.03021},
    language = {en},
    number = {60},
    urldate = {2025-06-07},
    journal = {Journal of Open Source Software},
    author = {Waskom, Michael L.},
    month = apr,
    year = {2021},
    pages = {3021},
}
@misc{GoogleColab,
    title = {Google {Colab}},
    url = {https://colab.research.google.com/},
    language = {en},
    urldate = {2025-06-07},
    journal = {Google Colab},
}
@misc{TimmPyTorchImage2025,
    title = {timm ({PyTorch} {Image} {Models})},
    url = {https://huggingface.co/timm},
    abstract = {Computer Vision},
    urldate = {2025-06-07},
    month = feb,
    year = {2025},
}
@misc{noauthor_data_nodate,
    title = {Data {Splitting}: {Everything} {You} {Need} to {Know} {When} {Assessing} {Data} {Splitting} {Skills}},
    shorttitle = {Data {Splitting}},
    url = {https://www.alooba.com/skills/concepts/deep-learning/data-splitting/},
    abstract = {Discover the power of data splitting with Alooba's comprehensive guide. Learn what data splitting is and how it enhances machine learning model accuracy in candidate assessment for large organizations. Boost your hiring process today!},
    language = {en},
    urldate = {2025-06-07},
}

@misc{garruchoDomainGeneralizationDeep2022,
    title = {Domain generalization in deep learning-based mass detection in mammography: {A} large-scale multi-center study},
    shorttitle = {Domain generalization in deep learning-based mass detection in mammography},
    url = {https://arxiv.org/abs/2201.11620v2},
    abstract = {Computer-aided detection systems based on deep learning have shown great potential in breast cancer detection. However, the lack of domain generalization of artificial neural networks is an important obstacle to their deployment in changing clinical environments. In this work, we explore the domain generalization of deep learning methods for mass detection in digital mammography and analyze in-depth the sources of domain shift in a large-scale multi-center setting. To this end, we compare the performance of eight state-of-the-art detection methods, including Transformer-based models, trained in a single domain and tested in five unseen domains. Moreover, a single-source mass detection training pipeline is designed to improve the domain generalization without requiring images from the new domain. The results show that our workflow generalizes better than state-of-the-art transfer learning-based approaches in four out of five domains while reducing the domain shift caused by the different acquisition protocols and scanner manufacturers. Subsequently, an extensive analysis is performed to identify the covariate shifts with bigger effects on the detection performance, such as due to differences in patient age, breast density, mass size, and mass malignancy. Ultimately, this comprehensive study provides key insights and best practices for future research on domain generalization in deep learning-based breast cancer detection.},
    language = {en},
    urldate = {2025-06-08},
    journal = {arXiv.org},
    author = {Garrucho, Lidia and Kushibar, Kaisar and Jouide, Socayna and Diaz, Oliver and Igual, Laura and Lekadir, Karim},
    month = jan,
    year = {2022},
    doi = {10.1016/j.artmed.2022.102386},
}
@book{marzbanAllModelsAre2020,
    title = {All {Models} {Are} {Wrong}: {Concepts} of {Statistical} {Learning}.},
    shorttitle = {All {Models} {Are} {Wrong}: {Concepts} of {Statistical} {Learning}.},
    url = {https://allmodelsarewrong.github.io},
    abstract = {This is a text about the fundamental concepts of Statistical Learning Methods.},
    urldate = {2025-06-08},
    author = {Marzban, Gaston Sanchez Ethan},
    year = {2020},
}
@misc{noauthor_complete_nodate,
    title = {A {Complete} {Guide} to {Data} {Augmentation}},
    url = {https://www.datacamp.com/tutorial/complete-guide-data-augmentation},
    abstract = {Learn about data augmentation techniques, applications, and tools with a TensorFlow and Keras tutorial.},
    language = {en},
    urldate = {2025-06-09},
}
@article{demsar_statistical_2006,
    title = {Statistical {Comparisons} of {Classifiers} over {Multiple} {Data} {Sets}},
    volume = {7},
    issn = {1532-4435},
    abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
    journal = {J. Mach. Learn. Res.},
    author = {Demšar, Janez},
    year = {2006},
    pages = {1--30},
}
@article{sinn_triple-negative_2023,
    title = {[{Triple}-negative breast cancer : {Classification}, current concepts, and therapy-related factors]},
    volume = {44},
    issn = {2731-7196},
    shorttitle = {[{Triple}-negative breast cancer},
    doi = {10.1007/s00292-022-01177-y},
    abstract = {Triple-negative breast cancer (TNBC) accounts for about 10\% of all breast cancer cases and is defined by the lack of expression of estrogen and progesterone receptors and the lack of overexpression or amplification of HER2. It differs with regard to the younger age of the patients, an increased association with a mutation of BRCA1 and a mostly low differentiation from hormone receptor-positive breast cancer. The spectrum of triple-negative breast cancer shows considerable heterogeneity both at the morphological and at the molecular level. It includes most commonly TNBC of no special type, with and without basal phenotype, triple-negative metaplastic breast carcinomas, triple-negative breast carcinomas with apocrine differentiation and rare triple-negative tumor types. At the gene-expression level, TNBC most commonly is associated with a basal phenotype, with rarer molecular variants of TNBC involving the Claudin-low, molecular apocrine types, and other rarer subtypes. Therefore, a critical use of the term TNBC, considering the histopathological tumor differentiation, is recommended.},
    language = {ger},
    number = {1},
    journal = {Pathologie (Heidelberg, Germany)},
    author = {Sinn, Hans-Peter and Varga, Zsuzsanna},
    month = feb,
    year = {2023},
    pmid = {36595080},
    keywords = {Apocrine breast carcinoma, Biomarkers, Biomarkers, Tumor, Humans, Low-malignant triple-negative breast carcinoma, Metaplastic breast carcinoma, Mutation, Receptor, ErbB-2, Salivary gland-type breast carcinoma, Triple Negative Breast Neoplasms},
    pages = {32--38},
}
@misc{noauthor_welcome_nodate,
    title = {Welcome to {OpenVINO}™ {Explainable} {AI} {Toolkit}’s documentation! — {OpenVINO}™ {XAI} 1.1.0 documentation},
    url = {https://openvinotoolkit.github.io/openvino_xai/stable/},
    urldate = {2025-06-10},
}
@misc{noauthor_captum_nodate,
    title = {Captum · {Model} {Interpretability} for {PyTorch}},
    url = {https://captum.ai/},
    abstract = {Model Interpretability for PyTorch},
    urldate = {2025-06-10},
}
@misc{noauthor_scipy_nodate,
    title = {{SciPy}},
    url = {https://scipy.org/},
    urldate = {2025-06-10},
}
@misc{noauthor_161002391_nodate,
    title = {[1610.02391] {Grad}-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
    url = {https://arxiv.org/abs/1610.02391},
    urldate = {2025-06-10},
}
@misc{byun_vit-reciprocam_2023,
    title = {{ViT}-{ReciproCAM}: {Gradient} and {Attention}-{Free} {Visual} {Explanations} for {Vision} {Transformer}},
    shorttitle = {{ViT}-{ReciproCAM}},
    url = {http://arxiv.org/abs/2310.02588},
    doi = {10.48550/arXiv.2310.02588},
    abstract = {This paper presents a novel approach to address the challenges of understanding the prediction process and debugging prediction errors in Vision Transformers (ViT), which have demonstrated superior performance in various computer vision tasks such as image classification and object detection. While several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and Recipro-CAM, have been extensively researched for Convolutional Neural Networks (CNNs), limited research has been conducted on ViT. Current state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques. In this work, we propose a new gradient-free visual explanation method for ViT, called ViT-ReciproCAM, which does not require attention matrix and gradient information. ViT-ReciproCAM utilizes token masking and generated new layer outputs from the target layer's input to exploit the correlation between activated tokens and network predictions for target classes. Our proposed method outperforms the state-of-the-art Relevance method in the Average Drop-Coherence-Complexity (ADCC) metric by \$4.58{\textbackslash}\%\$ to \$5.80{\textbackslash}\%\$ and generates more localized saliency maps. Our experiments demonstrate the effectiveness of ViT-ReciproCAM and showcase its potential for understanding and debugging ViT models. Our proposed method provides an efficient and easy-to-implement alternative for generating visual explanations, without requiring attention and gradient information, which can be beneficial for various applications in the field of computer vision.},
    urldate = {2025-06-10},
    publisher = {arXiv},
    author = {Byun, Seok-Yong and Lee, Wonju},
    month = oct,
    year = {2023},
    note = {arXiv:2310.02588 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
@misc{panambur_classification_2023,
    title = {Classification of {Luminal} {Subtypes} in {Full} {Mammogram} {Images} {Using} {Transfer} {Learning}},
    url = {http://arxiv.org/abs/2301.09282},
    doi = {10.48550/arXiv.2301.09282},
    abstract = {Automatic identification of patients with luminal and non-luminal subtypes during a routine mammography screening can support clinicians in streamlining breast cancer therapy planning. Recent machine learning techniques have shown promising results in molecular subtype classification in mammography; however, they are highly dependent on pixel-level annotations, handcrafted, and radiomic features. In this work, we provide initial insights into the luminal subtype classification in full mammogram images trained using only image-level labels. Transfer learning is applied from a breast abnormality classification task, to finetune a ResNet-18-based luminal versus non-luminal subtype classification task. We present and compare our results on the publicly available CMMD dataset and show that our approach significantly outperforms the baseline classifier by achieving a mean AUC score of 0.6688 and a mean F1 score of 0.6693 on the test dataset. The improvement over baseline is statistically significant, with a p-value of p{\textless}0.0001.},
    urldate = {2025-06-10},
    publisher = {arXiv},
    author = {Panambur, Adarsh Bhandary and Madhu, Prathmesh and Maier, Andreas},
    month = jan,
    year = {2023},
    note = {arXiv:2301.09282 [eess]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}
@misc{noauthor_que_nodate,
    title = {¿{Qué} es una matriz de confusión? {\textbar} {IBM}},
    url = {https://www.ibm.com/es-es/think/topics/confusion-matrix},
    urldate = {2025-06-10},
}